{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading modules\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import pickle as pkl\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "print(\"Reloading modules\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data that is actally used by RNNTraining -> Our target format.\n",
    "small_dbtrain = loadmat('./coco/dbval_small.mat')\n",
    "db = small_dbtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <type 'dict'>\n",
      "Number of keys: 1003\n",
      "An example of key:9077\n",
      "Type of element 0 :<type 'numpy.ndarray'>\n",
      "Shape of element 0 :(1, 1)\n",
      "One value (one key):\n",
      "(array([ u'A variety of fruit is displayed in a market.                                           ',\n",
      "       u'Fruits and vegetables for sale in a farmers market.                                    ',\n",
      "       u'Many fruits displayed in a produce store including bananas, oranges, apples and lemons.',\n",
      "       u'The produce section of a grocery store showing many various fruits.                    ',\n",
      "       u'A market has many fruits and vegetables out for display.                               '],\n",
      "      dtype='<U87'), array([[ 172.,   45.,  640.,  427.],\n",
      "       [ 197.,   62.,  540.,  427.],\n",
      "       [  22.,   45.,  589.,  427.],\n",
      "       ..., \n",
      "       [ 135.,  265.,  160.,  359.],\n",
      "       [ 285.,   61.,  320.,   96.],\n",
      "       [  66.,  186.,  124.,  244.]], dtype=float32), array([[255, 216, 255, ..., 143, 255, 217]], dtype=uint8))\n",
      "First element: [ u'A variety of fruit is displayed in a market.                                           '\n",
      " u'Fruits and vegetables for sale in a farmers market.                                    '\n",
      " u'Many fruits displayed in a produce store including bananas, oranges, apples and lemons.'\n",
      " u'The produce section of a grocery store showing many various fruits.                    '\n",
      " u'A market has many fruits and vegetables out for display.                               ']\n",
      "Shape of the second element: (10000, 4)\n",
      "[ 172.   45.  640.  427.]\n"
     ]
    }
   ],
   "source": [
    "# DBTRAIN: What we must produce.\n",
    "num = 0 # Index of the image in the list.\n",
    "print(\"Type: \" + str(type(small_dbtrain)))\n",
    "print(\"Number of keys: \" + str(len(small_dbtrain.keys())))\n",
    "print(\"An example of key:\" + str(small_dbtrain.keys()[0]))\n",
    "print(\"Type of element \" + str(num) +\" :\" + str(type(small_dbtrain[small_dbtrain.keys()[num]])))\n",
    "print(\"Shape of element \" + str(num) +\" :\" + str(np.shape(small_dbtrain[small_dbtrain.keys()[num]])))\n",
    "one_element = small_dbtrain[small_dbtrain.keys()[num]][0][0]\n",
    "print(\"One value (one key):\")\n",
    "print(one_element)\n",
    "print(\"First element: \" + str(one_element[0]))\n",
    "print(\"Shape of the second element: \" + str(np.shape(one_element[1])))\n",
    "print(one_element[1][0,:])\n",
    "# GOAL: a dict with image_id as key containing a tuple with (sentences, numpy array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prying that resembles the one in RNNTraining\n",
    "#val = db.iteritems().next()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flickdata = json.load(open('./flickr30k/flickr30k/dataset.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'dict'>\n",
      "Keys: [u'images', u'dataset']\n",
      "Number of images in List: 31014\n",
      "Keys for one image: [u'filename', u'imgid', u'sentences', u'split', u'sentids']\n",
      "Split for an image: train\n"
     ]
    }
   ],
   "source": [
    "# Analysis of what we have as data\n",
    "num=0\n",
    "print(type(flickdata))\n",
    "print(\"Keys: \" + str(flickdata.keys()))\n",
    "flick_images = flickdata['images']\n",
    "print(\"Number of images in List: \" + str(len(flick_images)))\n",
    "print(\"Keys for one image: \" + str(flick_images[num].keys()))\n",
    "print(\"Split for an image: \" + str(flick_images[num]['split']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Taking a look at flickr matrix, let's see if maybe that's what we want\n",
    "vgg_flick = loadmat('./flickr30k/flickr30k/vgg_feats.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of vgg_flick: <type 'dict'>\n",
      "Keys: ['feats']\n",
      "Keys must have been deleted already\n",
      "(4096, 31014)\n"
     ]
    }
   ],
   "source": [
    "print(\"Type of vgg_flick: \" + str(type(vgg_flick)))\n",
    "print(\"Keys: \" + str(vgg_flick.keys()))\n",
    "#vgg_feats = {key: vgg_flick[key]['feats'] for key in vgg_flick.keys()}\n",
    "try:\n",
    "    del vgg_flick['__version__']\n",
    "    del vgg_flick['__header__']\n",
    "    del vgg_flick['__globals__']\n",
    "except:\n",
    "    print(\"Keys must have been deleted already\")\n",
    "vgg_feats = vgg_flick['feats']\n",
    "print(np.shape(vgg_feats))\n",
    "print(\"Conclusion: If we decide to trust it, vgg_flick seems to contain the features we want.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'info', u'images', u'licenses', u'annotations']\n"
     ]
    }
   ],
   "source": [
    "#annotations AND IMAGES used with COCO.\n",
    "ann_path = \"coco/annotations/\"\n",
    "split = 'train'\n",
    "ann=json.load(open(ann_path+'captions_%s2014.json'%split))\n",
    "print(ann.keys())\n",
    "annotations=ann['annotations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "{u'license': 5, u'file_name': u'COCO_train2014_000000057870.jpg', u'coco_url': u'http://mscoco.org/images/57870', u'height': 480, u'width': 640, u'date_captured': u'2013-11-14 16:28:13', u'flickr_url': u'http://farm4.staticflickr.com/3153/2970773875_164f0c0b83_z.jpg', u'id': 57870}\n"
     ]
    }
   ],
   "source": [
    "print(type(annotations))\n",
    "print(ann['images'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of annotations: 414113\n",
      "[u'image_id', u'id', u'caption']\n",
      "A very clean and well decorated empty bathroom\n",
      "318556\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of annotations: \" + str(len(annotations))) # More than one annotation per image..\n",
    "print(annotations[0].keys()) \n",
    "print(annotations[0]['caption']) \n",
    "print(annotations[0]['image_id']) \n",
    "print(annotations[0]['id'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Analysis of captions produced when training on coco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'images', u'dataset']\n",
      "flickr30k\n"
     ]
    }
   ],
   "source": [
    "# Analysis of annotations provided with the flick\n",
    "info = json.load(open('./flickr30k/flickr30k/dataset.json'))\n",
    "print(info.keys())\n",
    "informations = info['images']\n",
    "print(info['dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'filename', u'imgid', u'sentences', u'split', u'sentids']\n",
      "<type 'int'>\n",
      "1000092795.jpg\n"
     ]
    }
   ],
   "source": [
    "print(informations[0].keys())\n",
    "info = informations[0]\n",
    "print(type(info['imgid']))\n",
    "print(info['filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Analysis of annotations.dict found in flickr30k: maybe Marco did it already?\n",
    "annot =  pkl.load(open('/scratch/clear/mpederso/git/ImCap/flickr30k/annotations.dict'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'annot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-b401b9d5de91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mannot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'annot' is not defined"
     ]
    }
   ],
   "source": [
    "print(type(annot))\n",
    "print(len(annot.keys()))\n",
    "k = annot.keys()[0]\n",
    "print(annot[k].keys())\n",
    "print(annot[k]['sentence'][0])\n",
    "print(annot[k]['idToLabel'][0][0][0])\n",
    "print(annot[k]['id'][0][0])\n",
    "print(annot[k]['id'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating the 'annotations' and 'images' fields.\n",
    "my_ann, my_im, count = [], [], 0\n",
    "for i in informations:\n",
    "    #my_im.append({'file_name': i['filename'], 'id': int(i['imgid']), 'split': i['split']})\n",
    "    my_im.append({'file_name': i['filename'], 'id': int(i['filename'].split('.')[0]), 'split': i['split']})\n",
    "    for sent in i['sentences']:\n",
    "       #my_ann.append({'image_id': int(i['imgid']) , 'id': count , 'caption': sent, 'split': i['split']}) \n",
    "       my_ann.append({'image_id': int(i['filename'].split('.')[0]), 'id': count , 'caption': sent['raw'], 'split': i['split']}) \n",
    "       count += 1\n",
    "assert(len(my_ann) == 5*len(informations)), \"Unexpected length...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31014\n",
      "[u'filename', u'imgid', u'sentences', u'split', u'sentids']\n",
      "1\n",
      "10002456.jpg\n"
     ]
    }
   ],
   "source": [
    "# Creating the 'images' field of my_ann.\n",
    "print(len(informations))\n",
    "print(informations[1].keys())\n",
    "print(informations[1]['imgid'])\n",
    "print(informations[1]['filename'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_ann, val_ann, test_ann = {}, {}, {}\n",
    "train_ann['annotations'] = [e for e in my_ann if e['split'] == 'train']\n",
    "train_ann['images'] = [e for e in my_im if e['split'] == 'train']\n",
    "val_ann['annotations'] = [e for e in my_ann if e['split'] == 'val']\n",
    "val_ann['images'] = [e for e in my_im if e['split'] == 'val']\n",
    "test_ann['annotations'] = [e for e in my_ann if e['split'] == 'test']\n",
    "test_ann['images'] = [e for e in my_im if e['split'] == 'test']\n",
    "end_len = len(train_ann['annotations']) + len(val_ann['annotations']) + len(test_ann['annotations']) \n",
    "assert(end_len == len(my_ann)), \"Lost annotation: \" + str(end_len) + \" / \" + str(len(my_ann))\n",
    "#assert(len(train_im) + len(val_im) + len(test_im) == len(my_im)), \"Lost images....\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1014\n"
     ]
    }
   ],
   "source": [
    "print(len(val_ann['images']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3 ... 2/3 ... 3/3 OK\n"
     ]
    }
   ],
   "source": [
    "with open('./flick_data/train_annot.json', 'w') as outfile:\n",
    "    json.dump(train_ann, outfile)\n",
    "print(\"1/3 ...\"),\n",
    "with open('./flick_data/val_annot.json', 'w') as outfile:\n",
    "    json.dump(val_ann, outfile)\n",
    "print(\"2/3 ...\"),\n",
    "with open('./flick_data/test_annot.json', 'w') as outfile:\n",
    "    json.dump(test_ann, outfile)\n",
    "print(\"3/3 OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Debugging caption evaluation.\n",
    "captionfile = json.load(open('./flick_data/debug_delme.json'))\n",
    "from CaptionEvaluation import evaluateCaptionsFlickr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "> /scratch/algorab/tlucas/save_new_imcap/newpull_imccap/imcap/coco-caption/pycocoevalcap/eval.py(38)evaluate()\n",
      "-> gts  = tokenizer.tokenize(gts)\n",
      "(Pdb) c\n",
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'reflen': 10033, 'guess': [10140, 9126, 8112, 7098], 'testlen': 10140, 'correct': [4503, 1055, 133, 10]}\n",
      "ratio: 1.01066480614\n",
      "Bleu_1: 0.444\n",
      "Bleu_2: 0.227\n",
      "Bleu_3: 0.094\n",
      "Bleu_4: 0.033\n",
      "computing METEOR score...\n",
      "METEOR: 0.102\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.314\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.020\n"
     ]
    }
   ],
   "source": [
    "res = evaluateCaptionsFlickr(captionfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Check the training files to see if id and file names are the same:\n",
    "dbtrain = pkl.load(open('./flick_data/dbtrain.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145558685\n",
      "['caption', 'image']\n",
      "[u'A young girl with a big smile, wearing brightly colored clothing helps prepare food in a narrow alley.', u'A girl wearing a blue floral patterned dress is sitting on the cement and sorting leaves.', u'This is a street in a poor area where two people are sorting through tobacco.', u'This lady in the print dress is stripping food for dinner.', u'A woman puts the firewood in order.']\n"
     ]
    }
   ],
   "source": [
    "k = dbtrain.keys()[0]\n",
    "print(k)\n",
    "print(dbtrain[k].keys())\n",
    "print(dbtrain[k]['caption'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
